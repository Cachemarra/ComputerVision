{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the digit recognition model\n",
    "\n",
    "We will use MNIST dataset for training as we only need 0-9 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#%% Libraries\n",
    "from Configurations.models.sudokunet import SudokuNet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the hyperparameters and load the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "INIT_LR = 1e-3 # Learning Rate\n",
    "EPOCHS = 10 # Number of epochs\n",
    "BS = 128 # Batch size\n",
    "\n",
    "# Load the MNIST dataset\n",
    "print(\"[INFO] Loading MNIST dataset...\")\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST preprocessing\n",
    "\n",
    "MNIST data comes as numpy arrays [28, 28]. \n",
    "We're going to: \n",
    "- format the data as batches with a new dimention to indicate is a grayscale image, \n",
    "- Normalize the values (i. e. dividing by /255.0)\n",
    "- Finally, convert the labels from integers to vectors.\n",
    "\n",
    "What the last step do is changing the encode from [1, 3, 2, 0] to ``[[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [1, 0, 0, 0]]``\n",
    "\n",
    "This is called One-Hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the grayscale channel\n",
    "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "# Size of trainX = (60000, 28, 28, 1)\n",
    "\n",
    "# Normalizing\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "\n",
    "# Convert the labels from integers to vectors\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.fit_transform(testY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model\n",
    "\n",
    "We load the SudokuNet and then compile it with our desire parameters.\n",
    "The optimizer will be Adam and the loss will be Categorical Cross Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Compiling the model...\n",
      "[INFO] Compiling Model\n"
     ]
    }
   ],
   "source": [
    "#%% Initializing the model\n",
    "print(\"[INFO] Compiling the model...\")\n",
    "opt = Adam(lr=INIT_LR)\n",
    "model = SudokuNet.build(width=28, height=28, depth=1, classes=10)\n",
    "\n",
    "print(\"[INFO] Compiling Model\")\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\", \"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "1875/1875 [==============================] - 193s 47ms/step - loss: 0.9142 - accuracy: 0.6854 - mse: 0.0397 - val_loss: 0.0673 - val_accuracy: 0.9795 - val_mse: 0.0031\n",
      "Epoch 2/9\n",
      "1875/1875 [==============================] - 72s 38ms/step - loss: 0.2405 - accuracy: 0.9282 - mse: 0.0108 - val_loss: 0.0476 - val_accuracy: 0.9861 - val_mse: 0.0022\n",
      "Epoch 3/9\n",
      "1875/1875 [==============================] - 81s 43ms/step - loss: 0.1777 - accuracy: 0.9474 - mse: 0.0079 - val_loss: 0.0405 - val_accuracy: 0.9875 - val_mse: 0.0019\n",
      "Epoch 4/9\n",
      "1227/1875 [==================>...........] - ETA: 23s - loss: 0.1567 - accuracy: 0.9528 - mse: 0.0071"
     ]
    }
   ],
   "source": [
    "#%% Training the model\n",
    "\n",
    "history = model.fit(\n",
    "    trainX, trainY, \n",
    "    validation_data=(testX, testY),\n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BS, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model\n",
    "\n",
    "Then we show the evaluations of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluating the model...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       980\n",
      "           1       0.99      1.00      1.00      1135\n",
      "           2       0.99      1.00      0.99      1032\n",
      "           3       1.00      0.99      0.99      1010\n",
      "           4       0.99      1.00      0.99       982\n",
      "           5       0.99      0.99      0.99       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       1.00      0.99      0.99      1028\n",
      "           8       0.99      0.99      0.99       974\n",
      "           9       1.00      0.97      0.98      1009\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "[INFO] Saving the model...\n"
     ]
    }
   ],
   "source": [
    "#%% Model evaluation\n",
    "print(\"[INFO] Evaluating the model...\")\n",
    "predictions = model.predict(testX)\n",
    "print(\n",
    "    classification_report(\n",
    "        testY.argmax(axis=1), \n",
    "        predictions.argmax(axis=1), \n",
    "        target_names=[str(x) for x in lb.classes_]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Serializing the model to disk\n",
    "print(\"[INFO] Saving the model...\")\n",
    "model.save(\"./Configurations/models/sudokunet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c21e2ad06d3fae75f452f0ce83f813ba84392946366d7202f73a50514fe02fdb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
